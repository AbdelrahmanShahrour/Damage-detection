{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5141688,"sourceType":"datasetVersion","datasetId":1155801}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\n#For Preprocessing\nimport re    # RegEx for removing non-letter characters\nimport nltk  # natural language processing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# For Building the model\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\n\n#For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\npd.options.plotting.backend = \"plotly\"\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import History\nfrom tensorflow.keras import losses\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow.keras.backend as K\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T10:22:35.236759Z","iopub.execute_input":"2024-05-11T10:22:35.237047Z","iopub.status.idle":"2024-05-11T10:22:51.856413Z","shell.execute_reply.started":"2024-05-11T10:22:35.237020Z","shell.execute_reply":"2024-05-11T10:22:51.855610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install pydotplus\n# !pip install graphviz\n!conda uninstall pyplot\n!conda uninstall pyplotplus\n!conda uninstall graphviz","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:46:36.121279Z","iopub.execute_input":"2024-05-11T10:46:36.121653Z","iopub.status.idle":"2024-05-11T11:09:07.469341Z","shell.execute_reply.started":"2024-05-11T10:46:36.121626Z","shell.execute_reply":"2024-05-11T11:09:07.468083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0 = pd.read_csv('/kaggle/input/bitcoin-tweets/Bitcoin_tweets.csv',chunksize=100000,lineterminator='\\n')\ndf = pd.concat(df0)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:23:05.742784Z","iopub.execute_input":"2024-05-11T10:23:05.743333Z","iopub.status.idle":"2024-05-11T10:24:16.593635Z","shell.execute_reply.started":"2024-05-11T10:23:05.743305Z","shell.execute_reply":"2024-05-11T10:24:16.592600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df[['text']][0:20000]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:24:25.819470Z","iopub.execute_input":"2024-05-11T10:24:25.820129Z","iopub.status.idle":"2024-05-11T10:24:26.973773Z","shell.execute_reply.started":"2024-05-11T10:24:25.820090Z","shell.execute_reply":"2024-05-11T10:24:26.972524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet2words(tweet):\n    ''' \n    Convert tweet text into a sequence of words \n    :tweet -> text data\n    '''\n    \n    # convert to lowercase\n    text = tweet.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:24:32.830411Z","iopub.execute_input":"2024-05-11T10:24:32.831078Z","iopub.status.idle":"2024-05-11T10:24:32.837617Z","shell.execute_reply.started":"2024-05-11T10:24:32.831045Z","shell.execute_reply":"2024-05-11T10:24:32.836669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleantext=[]\nfor item in tqdm(df['text']):\n    words=tweet2words(item)\n    cleantext+=[words]\ndf['cleantext']=cleantext\ndf","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:24:48.835742Z","iopub.execute_input":"2024-05-11T10:24:48.836062Z","iopub.status.idle":"2024-05-11T10:25:51.582041Z","shell.execute_reply.started":"2024-05-11T10:24:48.836039Z","shell.execute_reply":"2024-05-11T10:25:51.581049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unlist(list):\n    words=''\n    for item in list:\n        words+=item+' '\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:25:55.817163Z","iopub.execute_input":"2024-05-11T10:25:55.817874Z","iopub.status.idle":"2024-05-11T10:25:55.822206Z","shell.execute_reply.started":"2024-05-11T10:25:55.817842Z","shell.execute_reply":"2024-05-11T10:25:55.821336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_vader_scores(df, label):\n    sid = SentimentIntensityAnalyzer()\n    df[\"vader_neg\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"neg\"])\n    df[\"vader_neu\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"neu\"])\n    df[\"vader_pos\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"pos\"])\n    df[\"vader_comp\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"compound\"])\n    df['cleantext2'] = df[label].apply(lambda x: unlist(x))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:26:01.950345Z","iopub.execute_input":"2024-05-11T10:26:01.951084Z","iopub.status.idle":"2024-05-11T10:26:01.959536Z","shell.execute_reply.started":"2024-05-11T10:26:01.951044Z","shell.execute_reply":"2024-05-11T10:26:01.957939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = compute_vader_scores(df,'cleantext')\ndf2","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:26:13.843052Z","iopub.execute_input":"2024-05-11T10:26:13.843432Z","iopub.status.idle":"2024-05-11T10:26:31.968486Z","shell.execute_reply.started":"2024-05-11T10:26:13.843402Z","shell.execute_reply":"2024-05-11T10:26:31.967412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data=df2, x='vader_pos', y='vader_neg', kind=\"kde\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:26:33.896741Z","iopub.execute_input":"2024-05-11T10:26:33.897096Z","iopub.status.idle":"2024-05-11T10:26:50.200512Z","shell.execute_reply.started":"2024-05-11T10:26:33.897068Z","shell.execute_reply":"2024-05-11T10:26:50.199516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data=df2, x='vader_pos', y='vader_neu', kind=\"kde\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:26:53.031880Z","iopub.execute_input":"2024-05-11T10:26:53.032686Z","iopub.status.idle":"2024-05-11T10:27:08.715940Z","shell.execute_reply.started":"2024-05-11T10:26:53.032648Z","shell.execute_reply":"2024-05-11T10:27:08.714993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class0=[]\nfor i in range(len(df2)):\n    if df2.loc[i,'vader_neg']>0:\n        class0+=[0]\n    elif df2.loc[i,'vader_pos']>0:\n        class0+=[2]        \n    else:\n        class0+=[1]  ","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:10.249474Z","iopub.execute_input":"2024-05-11T10:27:10.250292Z","iopub.status.idle":"2024-05-11T10:27:10.830384Z","shell.execute_reply.started":"2024-05-11T10:27:10.250258Z","shell.execute_reply":"2024-05-11T10:27:10.829177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class']=class0\ndf['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:13.164209Z","iopub.execute_input":"2024-05-11T10:27:13.165132Z","iopub.status.idle":"2024-05-11T10:27:13.188555Z","shell.execute_reply.started":"2024-05-11T10:27:13.165095Z","shell.execute_reply":"2024-05-11T10:27:13.187120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', df['cleantext2'][0])\nX, tokenizer = tokenize_pad_sequences(df['cleantext2'])\nprint('After Tokenization & Padding \\n', X[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:14.697627Z","iopub.execute_input":"2024-05-11T10:27:14.697987Z","iopub.status.idle":"2024-05-11T10:27:15.641256Z","shell.execute_reply.started":"2024-05-11T10:27:14.697959Z","shell.execute_reply":"2024-05-11T10:27:15.640374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:17.957592Z","iopub.execute_input":"2024-05-11T10:27:17.957979Z","iopub.status.idle":"2024-05-11T10:27:17.963257Z","shell.execute_reply.started":"2024-05-11T10:27:17.957952Z","shell.execute_reply":"2024-05-11T10:27:17.962225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = pd.get_dummies(df['class'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\nprint('Train Set: ', X_train.shape, y_train.shape)\nprint('Validation Set: ', X_val.shape, y_val.shape)\nprint('Test Set: ', X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:19.367570Z","iopub.execute_input":"2024-05-11T10:27:19.367954Z","iopub.status.idle":"2024-05-11T10:27:19.386009Z","shell.execute_reply.started":"2024-05-11T10:27:19.367923Z","shell.execute_reply":"2024-05-11T10:27:19.385029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:20.209881Z","iopub.execute_input":"2024-05-11T10:27:20.210983Z","iopub.status.idle":"2024-05-11T10:27:20.216404Z","shell.execute_reply.started":"2024-05-11T10:27:20.210942Z","shell.execute_reply":"2024-05-11T10:27:20.215491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 5000\nembedding_size = 32\nepochs = 10\nlearning_rate = 0.1\ndecay_rate = learning_rate / epochs\nmomentum = 0.8","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:27:21.059054Z","iopub.execute_input":"2024-05-11T10:27:21.059827Z","iopub.status.idle":"2024-05-11T10:27:21.064351Z","shell.execute_reply.started":"2024-05-11T10:27:21.059790Z","shell.execute_reply":"2024-05-11T10:27:21.063347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.01,\n    decay_steps=10000,\n    decay_rate=0.9)\nsgd = tf.keras.optimizers.Adam(learning_rate=lr_schedule)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:34:56.393539Z","iopub.execute_input":"2024-05-11T10:34:56.394339Z","iopub.status.idle":"2024-05-11T10:34:56.401217Z","shell.execute_reply.started":"2024-05-11T10:34:56.394307Z","shell.execute_reply":"2024-05-11T10:34:56.400016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model Part.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD\n\n\n\nsgd = SGD(0.1, momentum=momentum, decay=decay_rate, nesterov=False)\n# Build model\nmodel= Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel.add(Conv1D(filters=32, kernel_size=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:35:30.861712Z","iopub.execute_input":"2024-05-11T10:35:30.862683Z","iopub.status.idle":"2024-05-11T10:35:30.894102Z","shell.execute_reply.started":"2024-05-11T10:35:30.862651Z","shell.execute_reply":"2024-05-11T10:35:30.893383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.utils import plot_model\n\n# plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:10:37.788701Z","iopub.execute_input":"2024-05-11T11:10:37.789493Z","iopub.status.idle":"2024-05-11T11:10:37.793894Z","shell.execute_reply.started":"2024-05-11T11:10:37.789461Z","shell.execute_reply":"2024-05-11T11:10:37.792680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:09:09.818626Z","iopub.execute_input":"2024-05-11T11:09:09.819015Z","iopub.status.idle":"2024-05-11T11:09:09.864183Z","shell.execute_reply.started":"2024-05-11T11:09:09.818976Z","shell.execute_reply":"2024-05-11T11:09:09.863078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train,y_train,validation_data=(X_val, y_val),batch_size=64,epochs=epochs,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:09:12.866858Z","iopub.execute_input":"2024-05-11T11:09:12.867220Z","iopub.status.idle":"2024-05-11T11:10:30.998999Z","shell.execute_reply.started":"2024-05-11T11:09:12.867190Z","shell.execute_reply":"2024-05-11T11:10:30.997912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:10:42.438280Z","iopub.execute_input":"2024-05-11T11:10:42.438656Z","iopub.status.idle":"2024-05-11T11:10:42.964195Z","shell.execute_reply.started":"2024-05-11T11:10:42.438627Z","shell.execute_reply":"2024-05-11T11:10:42.963248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1,2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    \n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:10:46.616032Z","iopub.execute_input":"2024-05-11T11:10:46.616417Z","iopub.status.idle":"2024-05-11T11:10:47.187186Z","shell.execute_reply.started":"2024-05-11T11:10:46.616377Z","shell.execute_reply":"2024-05-11T11:10:47.186266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes = ['Negative','Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(y_pred, axis=1),np.argmax(np.array(y_test),axis=1))\n    \n    print(pd.Series(np.argmax(np.array(y_test),axis=1)).value_counts())\n    print(pd.Series(np.argmax(y_pred, axis=1)).value_counts())\n    \n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:10:50.181600Z","iopub.execute_input":"2024-05-11T11:10:50.182520Z","iopub.status.idle":"2024-05-11T11:10:51.229270Z","shell.execute_reply.started":"2024-05-11T11:10:50.182485Z","shell.execute_reply":"2024-05-11T11:10:51.228315Z"},"trusted":true},"execution_count":null,"outputs":[]}]}